{"nbformat":4,"nbformat_minor":0,"metadata":{"anaconda-cloud":{},"colab":{"name":"JAIN_MAYANK_113263864_hw5-P1.ipynb","provenance":[{"file_id":"1V9BZ6XGvCjPG4jkAya92LC0NLbU4OM1l","timestamp":1586639198919}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"name":"HW3.ipynb"},"cells":[{"cell_type":"markdown","metadata":{"ein.tags":"worksheet-0","id":"hVwJj1nUL4lx"},"source":["# CSE527 Homework 5\n","**Due date: 23:59 on May 8, 2021 (Saturday)**\n","\n","In this semester, we will use Google Colab for the assignments, which allows us to utilize resources that some of us might not have in their local machines such as GPUs. You will need to use your Stony Brook (*.stonybrook.edu) account for coding and Google Drive to save your results.\n","\n","## Google Colab Tutorial\n","---\n","Go to https://colab.research.google.com/notebooks/, you will see a tutorial named \"Welcome to Colaboratory\" file, where you can learn the basics of using google colab.\n","\n","Settings used for assignments: ***Edit -> Notebook Settings -> Runtime Type (Python 3)***.\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k4dhRR_119C4","executionInfo":{"status":"ok","timestamp":1620600811419,"user_tz":240,"elapsed":8939,"user":{"displayName":"Mayank Jain","photoUrl":"","userId":"13340315988926188509"}},"outputId":"a5d01f12-3516-4bed-9972-4304770debbf"},"source":["pip install opencv-contrib-python==3.4.2.17"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting opencv-contrib-python==3.4.2.17\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/12/32/8d32d40cd35e61c80cb112ef5e8dbdcfbb06124f36a765df98517a12e753/opencv_contrib_python-3.4.2.17-cp37-cp37m-manylinux1_x86_64.whl (30.6MB)\n","\u001b[K     |████████████████████████████████| 30.6MB 163kB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-contrib-python==3.4.2.17) (1.19.5)\n","Installing collected packages: opencv-contrib-python\n","  Found existing installation: opencv-contrib-python 4.1.2.30\n","    Uninstalling opencv-contrib-python-4.1.2.30:\n","      Successfully uninstalled opencv-contrib-python-4.1.2.30\n","Successfully installed opencv-contrib-python-3.4.2.17\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VRU7ngb-2Jye","executionInfo":{"status":"ok","timestamp":1620600832305,"user_tz":240,"elapsed":17634,"user":{"displayName":"Mayank Jain","photoUrl":"","userId":"13340315988926188509"}},"outputId":"2f1f678e-775f-4893-9e9b-fd3405640ff6"},"source":["# Mount your google drive where you've saved your assignment folder\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LjAojc6I2QIa","executionInfo":{"status":"ok","timestamp":1620600870343,"user_tz":240,"elapsed":409,"user":{"displayName":"Mayank Jain","photoUrl":"","userId":"13340315988926188509"}},"outputId":"9c6c3a21-ac09-417f-8a79-1b768fcbd364"},"source":["# Set your working directory (in your google drive)\n","#   change it to your specific homework directory.\n","%cd '/content/gdrive/My Drive/JAIN_MAYANK_113263864_hw5'\n"],"execution_count":3,"outputs":[{"output_type":"stream","text":["/content/gdrive/My Drive/JAIN_MAYANK_113263864_hw5\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"p-UnOMINVrKQ"},"source":["## Problem 1: Ray casting using Blinn-Phong model\n","\n","\n","## Description\n","----\n","In this problem, you will be estimating the camera captured image given the camera parameters, scene geometry, lighting and materials using \n","[Blinn-Phong illumination model](https://dl.acm.org/doi/abs/10.1145/563858.563893), an improved version of [Phong illumination model](https://dl.acm.org/doi/abs/10.1145/360825.360839).\n","\n","The six parts below will guide you to render an image using ray casting and Phong model step by step."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TxrQ1z4sObHr","executionInfo":{"status":"ok","timestamp":1620600874598,"user_tz":240,"elapsed":419,"user":{"displayName":"Mayank Jain","photoUrl":"","userId":"13340315988926188509"}},"outputId":"4bd056dc-e753-4091-e433-050349f9b845"},"source":["# import packages here\n","import numpy as np\n","import cv2 as cv\n","from math import sqrt, fmod, pi\n","import matplotlib.pyplot as plt\n","\n","print(cv.__version__) # verify OpenCV version\n","\n","# figure and show\n","def imshow(im, title=None):\n","    im = im.squeeze()\n","\n","    # remove white paddings\n","    fig = plt.figure()\n","    # fig.canvas.window().statusBar().setVisible(False)\n","\n","    # display image\n","    ax = plt.imshow(im, interpolation='bilinear')\n","    plt.axis('off')\n","    plt.tight_layout(pad=0)\n","    plt.subplots_adjust(left=0, right=1, top=1, bottom=0, hspace=0, wspace=0)\n","    plt.margins(0, 0)\n","    plt.gca().xaxis.set_major_locator(plt.NullLocator())\n","    plt.gca().yaxis.set_major_locator(plt.NullLocator())\n","    if title is not None:\n","        plt.title(title)\n","        plt.subplots_adjust(left=0, right=1, top=0.9, bottom=0, hspace=0, wspace=0)\n","    plt.show()\n","    return fig\n","\n","# A 3x1 vector class extended from np.ndarray\n","class Vec3(np.ndarray):\n","    def __new__(cls, x, y=None, z=None):\n","        if y is None and z is None:\n","            data = np.array([x, x, x])\n","        else:\n","            data = np.array([x, y, z])\n","        return data.view(cls)\n","\n","    # vector normal\n","    def norm(self):\n","        return np.linalg.norm(self)\n","\n","    # normalized vector\n","    def normalize(self):\n","        return self / self.norm()\n","\n","    # vector cross product\n","    def cross(self, b):\n","        return np.cross(self, b)"],"execution_count":4,"outputs":[{"output_type":"stream","text":["3.4.2\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"hdhZr1B_Wvko"},"source":["### Part 1: Compute camera intrinsic matrix and camera ray\n","(15 points) Compute the 3x3 camera intrinsic matrix from the given camera image size, sensor size and focal length. Then, for each camera image pixel, we shoot a camera ray from the camera optical center (ray origin) and pass it through the pixel. Implement your alogrithm in `cam_ray_dir`. \n","\n","**Hint**: refer to slides, you will compute each camera ray direction using inverse of camera matrix.\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z45QddFWWRII","executionInfo":{"status":"ok","timestamp":1620600882022,"user_tz":240,"elapsed":436,"user":{"displayName":"Mayank Jain","photoUrl":"","userId":"13340315988926188509"}},"outputId":"4590ef6b-a005-4bf3-a836-da5449744186"},"source":["# create a perspective camera model. DO NOT MODIFY them\n","cam_pos = Vec3(0, 0, 0)\n","cam_h, cam_w = (500, 500)  # camera image size (pixel)\n","sensor_h, sensor_w = (36, 36)  # camera sensor size (mm)\n","focal_length = 50  # camera lens focal length (mm)\n","\n","fx, fy = (focal_length / sensor_w * cam_w, focal_length / sensor_h * cam_h)\n","cx, cy = (cam_w / 2, cam_h / 2)\n","cam_k = np.array([[fx, 0, cx], [0, fy, cy], [0, 0, 1]])  # camera intrinsics\n","\n","inv_cam_k = np.linalg.inv(cam_k)  # precompute inverse of camera intrinsics\n","\n","# compute the direction of the camera ray that passes through camera optical center (ray origin) and pixel (x, y)\n","def cam_ray_dir(inv_cam_k, cam_pos, focal_length, x, y):\n","  ray_dir = None\n","\n","  ##########--WRITE YOUR CODE HERE--##########\n","  ray_dir = Vec3(0, 0, 0)\n","  dot_product = np.dot(inv_cam_k,Vec3(x, y, 1))\n","  prod_of_focal_length_andDotProduct = focal_length * dot_product\n","  ray_dir = prod_of_focal_length_andDotProduct - cam_pos\n","  ray_dir = ray_dir.normalize()\n","  ##########-------END OF CODE-------##########\n","  return ray_dir\n","\n","print('camera intrinsics = \\n', cam_k)"],"execution_count":5,"outputs":[{"output_type":"stream","text":["camera intrinsics = \n"," [[694.44444444   0.         250.        ]\n"," [  0.         694.44444444 250.        ]\n"," [  0.           0.           1.        ]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"IqByBVKMXYW4"},"source":["### Part 2: Compute ray-object intersection\n","(15 points) For each camera ray, you will compute the intersection point of the camera ray with each object in the scene and store the intersection information in an `Intersection` object. Implement sphere-ray intersections, plane-ray intersections and ray-scene intersections below."]},{"cell_type":"code","metadata":{"id":"QpMMWA71WLsa","executionInfo":{"status":"ok","timestamp":1620600903659,"user_tz":240,"elapsed":437,"user":{"displayName":"Mayank Jain","photoUrl":"","userId":"13340315988926188509"}}},"source":["# A class used to store ray-object intersection information\n","class Intersection(object):\n","    def __init__(self, point, distance, normal, obj):\n","        self.p = point  # intersection point/position\n","        self.d = distance  # distance between this intersection and ray origin\n","        self.n = normal  # surface normal at this intersection\n","        self.obj = obj  # which object this intersection belongs to\n","\n","# Light ray class\n","class Ray(object):\n","    def __init__(self, origin, direction):\n","        self.o = origin\n","        self.d = direction\n","\n","    # ray-object intersection\n","    def intersect(self, obj):\n","        return obj.intersect(self)\n","\n","    # ray intersect scene i.e., check intersections with all objects.\n","    # return the closest if at lease one intersection, otherwise return none\n","    def intersect_scene(self, scene, ignored_obj=None):\n","        inter = None # returned inter is an instance of Intersection\n","        \n","        ##########--WRITE YOUR CODE HERE--##########\n","        object_intersections = []\n","        \n","        for s in scene:\n","          object_intersections.append(s.intersect(self))\n","        \n","        minimum_direction_value = np.inf\n","\n","        for intersect in object_intersections:\n","          if intersect is not None:\n","            if minimum_direction_value > intersect.d :\n","              minimum_direction_value = intersect.d\n","              inter = intersect\n","\n","        ##########-------END OF CODE-------##########\n","       \n","        return inter\n","\n","# Sphere class\n","class Sphere(object):\n","    def __init__(self, center, radius, material=None):\n","        self.c = center\n","        self.r = radius\n","        self.rr = radius ** 2  # precompute r^2 for faster ray casting\n","        self.mat = material  # Phong material\n","\n","    def intersect(self, l):  # intersect with ray l\n","        inter = None # returned inter is an instance of Intersection\n","        # solve quadratic equation of ray-sphere intersection\n","\n","        ##########--WRITE YOUR CODE HERE--##########\n","        quad_coeff = np.dot(l.d, l.d)\n","        linear_coeff = np.dot((l.o - self.c), l.d) * 2\n","        constant = np.dot((l.o - self.c), (l.o - self.c)) - self.rr\n","        discriminant = pow(linear_coeff,2) - 4 * quad_coeff * constant\n","        \n","        if discriminant < 0:\n","          return None\n","    \n","        root1 = (-1 * linear_coeff + pow(discriminant,0.5)) / (2 * quad_coeff)\n","        root2 = (-1 * linear_coeff - pow(discriminant,0.5)) / (2 * quad_coeff)\n","        \n","        if root1 < 0 and root2 < 0:\n","          return None\n","\n","        a_intersection = l.o + l.d * root1\n","        b_intersection = l.o + l.d * root2\n","\n","        if np.linalg.norm(l.o - a_intersection) <= np.linalg.norm(l.o - b_intersection):\n","          intersection_origin_dist = np.linalg.norm(l.o - a_intersection)\n","          s_normal = (a_intersection - self.c).normalize()\n","          inter = Intersection(a_intersection, intersection_origin_dist, s_normal, self)\n","          \n","        else:\n","          intersection_origin_dist = np.linalg.norm(l.o - b_intersection)\n","          s_normal = (b_intersection - self.c).normalize()\n","          inter = Intersection(b_intersection, intersection_origin_dist, s_normal, self)\n","\n","        ##########-------END OF CODE-------##########\n","        return inter\n","\n","\n","# Plane class\n","class Plane(object):\n","    def __init__(self, point, normal, material=None):\n","        self.p = point  # a point on the plane\n","        self.n = normal  # must be a normalized Vec3\n","        self.mat = material  # Phong material\n","\n","    def intersect(self, l):  # intersect with ray l\n","        inter = None # returned inter is an instance of Intersection\n","\n","        ##########--WRITE YOUR CODE HERE--##########\n","        numerator = np.dot((self.p - l.o), self.n)\n","        denominator = np.dot(l.d, self.n)\n","        res = numerator/denominator\n","        \n","        if res < 0:\n","          return None\n","\n","        intersection = (l.d * res) + l.o\n","        dist = np.linalg.norm(l.o - intersection)\n","        inter = Intersection(intersection, dist, self.n, self)\n","\n","        ##########-------END OF CODE-------##########\n","\n","        return inter"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3pqN9XguYHZb"},"source":["### Part 3: Render a vanilla ray casting image\n","(15 points) This part aims to help you sanity check the ray-object intersection algorithms above by rendering an image with `vanilla_ray_cast `. You will set a camera image pixel to (0.5,0.5,0.5) if its corresponding camera ray hits an object in the scene, otherwise set it to (0,0,0). You will compute the ray direction using `cam_ray_dir` and create a variable `ray` using `Ray(origin, dir)`, then `vanilla_ray_cast` will compute the current pixel color."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":523},"id":"-Y64PmH1O-5m","executionInfo":{"status":"ok","timestamp":1620600933059,"user_tz":240,"elapsed":24233,"user":{"displayName":"Mayank Jain","photoUrl":"","userId":"13340315988926188509"}},"outputId":"bf2b75dc-5605-4d1f-9e97-6f7b1f573917"},"source":["#  Only check if ray intersects the objects or not, does not computer color\n","def vanilla_ray_cast(ray, objects):\n","    inter = ray.intersect_scene(objects)\n","    if inter is None:\n","        color = Vec3(0, 0, 0)\n","    else:\n","        color = Vec3(1, 1, 1)*0.5\n","    return color\n","    \n","# initialize scene with three spheres\n","scene = []  # scene stores a list of objects\n","scene.append(Sphere(Vec3(12, 9, 600), 50))\n","scene.append(Sphere(Vec3(-30, 100, 550), 60))\n","scene.append(Sphere(Vec3(80, 95, 630), 65))\n","\n","# camera captured image\n","img = np.zeros((cam_h, cam_w, 3))\n","\n","# render using ray casting\n","print('Ray casting...')\n","for x in range(cam_w):  # for each col\n","    if fmod(x, cam_w // 10) == 0:  # print ray casting progress\n","        print('{:.2f}%'.format(x / cam_w * 100))\n","    for y in range(cam_h):  # for each row\n","\n","        ##########--WRITE YOUR CODE HERE--##########     \n","        cameraRayCast = cam_ray_dir(inv_cam_k, cam_pos, focal_length, x, y)\n","        ray = Ray(cam_pos, cameraRayCast)   \n","\n","        ##########-------END OF CODE-------##########\n","\n","        color = vanilla_ray_cast(ray, scene)\n","        img[y, x] = color\n","\n","# apply gamma correction (1/2.2) for display\n","img = np.power(img, 1 / 2.2)\n","imshow(img)\n","cv.imwrite('vanilla_ray_cast.png', img[..., ::-1] * 255)\n","\n"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Ray casting...\n","0.00%\n","10.00%\n","20.00%\n","30.00%\n","40.00%\n","50.00%\n","60.00%\n","70.00%\n","80.00%\n","90.00%\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"markdown","metadata":{"id":"ODICd6ZraNbS"},"source":["### Part 4: Render a Lambertian ray casting image\n","(15 points) If Part 3 is correctly implemented, you can estimate a simple lambertian shadings of the scene (add light, material but ignore shadow, ambient and specular). \n","Implement your algorithm in `diffuseColor`. Since we are dealing with colors, Phong materials are created and each object is assigned with a material."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eckCzBy_qFfk","outputId":"b7ec3b97-0e34-4a9c-a21d-abd047434adb"},"source":["# Light source (point light)\n","class Light(object):\n","    def __init__(self, position, power, ambient, diffuse, specular):\n","        self.p = position  # position, Vec3\n","        self.power = power  # light intensity/brightness\n","        self.ambient = ambient\n","        self.diffuse = diffuse\n","        self.specular = specular\n","\n","# Phong diffuse (lambertian) color\n","def diffuseColor(inter, light):\n","    color = 0\n","\n","    ##########--WRITE YOUR CODE HERE--##########\n","    normalized_diffOfPoints = (light.p - inter.p).normalize()\n","    prod_point_and_normal = np.dot(normalized_diffOfPoints, inter.n)\n","    color = inter.obj.mat['kd'] * prod_point_and_normal * light.power * light.diffuse\n","\n","    ##########-------END OF CODE-------##########\n","    \n","    return color\n","\n","#  Only compute lambertian (diffuse) color, no ambient or shadow\n","def lambertian_ray_cast(ray, objects, light):\n","    inter = ray.intersect_scene(objects)\n","    if inter is None:\n","        color = Vec3(0, 0, 0)\n","    else:\n","        d = (light.p - inter.p).norm()\n","        color = np.clip(diffuseColor(inter, light) / (4 * pi * d * d), 0, 1)\n","    return color\n","\n","# Render\n","# Phong illumination materials\n","plaster_white = {'ka': Vec3(0.03), 'kd': Vec3(0.5), 'ks': Vec3(0.1), 'shine': 5}\n","plaster_red = {'ka': Vec3(0.9, 0.1, 0.1) * 0.01, 'kd': Vec3(0.9, 0.1, 0.1), 'ks': Vec3(0.1), 'shine': 5}\n","plaster_green = {'ka': Vec3(0.1, 0.9, 0.1) * 0.01, 'kd': Vec3(0.1, 0.9, 0.1), 'ks': Vec3(0.1), 'shine': 5}\n","marble_white = {'ka': Vec3(0.01), 'kd': Vec3(0.8), 'ks': Vec3(1.0), 'shine': 200.0}\n","marble_yellow = {'ka': Vec3(0.8, 0.8, 0.1) * 0.1, 'kd': Vec3(0.8, 0.8, 0.1), 'ks': Vec3(1.0), 'shine': 200.0}\n","plastic_red = {'ka': Vec3(0.7, 0.1, 0.2) * 0.01, 'kd': Vec3(0.7, 0.1, 0.2), 'ks': Vec3(0.2), 'shine': 20}\n","\n","# initialize scene with spheres and a box\n","scene = []  # scene stores a list of objects\n","# a box\n","scene.append(Plane(Vec3(0, -160, 0), Vec3(0, 1, 0), plaster_white))  # ceiling\n","scene.append(Plane(Vec3(0, 160, 0), Vec3(0, -1, 0), plaster_white))  # floor\n","scene.append(Plane(Vec3(-160, 0, 0), Vec3(1, 0, 0), plaster_red))  # left\n","scene.append(Plane(Vec3(160, 0, 0), Vec3(-1, 0, 0), plaster_green))  # right\n","scene.append(Plane(Vec3(0, 0, 780), Vec3(0, 0, -1), plaster_white))  # back\n","# spheres\n","scene.append(Sphere(Vec3(12, 9, 600), 50, marble_white))\n","scene.append(Sphere(Vec3(-30, 100, 550), 60, marble_yellow))\n","scene.append(Sphere(Vec3(80, 95, 630), 65, plastic_red))\n","\n","# initialize a point light\n","light = Light(Vec3(-68, -68, 400), 300000, Vec3(1.0), Vec3(1.0), Vec3(1.0))\n","\n","# camera captured image\n","img = np.zeros((cam_h, cam_w, 3))\n","\n","# render using ray casting\n","print('Ray casting...')\n","for x in range(cam_w):  # for each col\n","    if fmod(x, cam_w // 10) == 0:  # print ray casting progress\n","        print('{:.2f}%'.format(x / cam_w * 100))\n","    for y in range(cam_h):  # for each row\n","\n","        ##########--WRITE YOUR CODE HERE--##########     \n","        \n","        cameraRayCast = cam_ray_dir(inv_cam_k, cam_pos, focal_length, x, y)\n","        ray = Ray(cam_pos, cameraRayCast)\n","\n","        ##########-------END OF CODE-------##########\n","\n","        color = lambertian_ray_cast(ray, scene, light)\n","        img[y, x] = color\n","\n","# apply gamma correction (1/2.2) for display\n","img = np.power(img, 1 / 2.2)\n","imshow(img)\n","cv.imwrite('lambertian_ray_cast.png', img[..., ::-1] * 255)\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Ray casting...\n","0.00%\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:99: RuntimeWarning: divide by zero encountered in true_divide\n"],"name":"stderr"},{"output_type":"stream","text":["10.00%\n","20.00%\n","30.00%\n","40.00%\n","50.00%\n","60.00%\n","70.00%\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"MzvdhAeOmhbs"},"source":["## What's more?\n","(0 points) For curious students\n","\n","1.   Is the above image photorealistic enough? How can we improve it?\n","  *  Add more lights, materials and objects.\n","  *  Add textures.\n","  *  Render triangle mesh.\n","  *  Acceleration techniques, e.g., depth clipping, bounding box, BVH, k-d tree, multithreading, GPU.\n","  *  Antialiasing.\n","  *  Depth of field.\n","  *  Add transparency, inter-reflection, volume scattering, caustics, etc.\n","\n","\n","2.   How do human visual systems tell whether an image is real or fake? Apparently shadow is an important cue.\n","3.   Inverse problem: can we estimate lighting (direction and color), object surface normal, material and camera parameters from the image above?\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"PhZehMVdL4mS"},"source":["## Submission guidelines\n","---\n","Extract the downloaded .zip file to a folder of your preference. The input and output paths are predefined and **DO NOT** change them, (we assume that 'Surname_Givenname_SBUID_hw5' is your working directory, and all the paths are relative to this directory).  The image read and write functions are already written for you. All you need to do is to fill in the blanks as indicated to generate proper outputs. **DO NOT** zip and upload the dataset on blackboard due to size limit.\n","\n","When submitting your .zip file through blackboard, please\n","-- name your .zip file as **Surname_Givenname_SBUID_hw*.zip**.\n","\n","This zip file should include:\n","```\n","Surname_Givenname_SBUID_hw*\n","        |---Surname_Givenname_SBUID_hw*.ipynb\n","        |---Surname_Givenname_SBUID_hw*.pdf\n","```\n","\n","For instance, student Michael Jordan should submit a zip file named \"Lecun_Yann_111134567_hw5.zip\" for homework5 in this structure:\n","```\n","Lecun_Yann_111134567_hw5\n","        |---Lecun_Yann_111134567_hw5.ipynb\n","        |---Lecun_Yann_111134567_hw5.pdf\n","```\n","\n","The **Surname_Givenname_SBUID_hw*.pdf** should include a **google shared link** and **Surname_Givenname_SBUID_Pred*.pdf** should be your test set prediction file in the specified format. To generate the **google shared link**, first create a folder named **Surname_Givenname_SBUID_hw*** in your Google Drive with your Stony Brook account. The structure of the files in the folder should be exactly the same as the one you downloaded. If you alter the folder structures, the grading of your homework will be significantly delayed and possibly penalized.\n","\n","Then right click this folder, click ***Get shareable link***, in the People textfield, enter TA's emails. Make sure that TAs who have the link **can edit**, ***not just*** **can view**, and also **uncheck** the **Notify people** box.\n","\n","Colab has a good feature of version control, you should take advantage of this to save your work properly. However, the timestamp of the submission made in blackboard is the only one that we consider for grading. To be more specific, we will only grade the version of your code right before the timestamp of the submission made in blackboard. \n","\n","You are encouraged to post and answer questions on Piazza. Based on the amount of email that we have received in past years, there might be dealys in replying to personal emails. Please ask questions on Piazza and send emails only for personal issues.\n","\n","Be aware that your code will undergo plagiarism check both vertically and horizontally. Please do your own work.\n","\n","**Late submission penalty:** <br>\n","There will be a 10% penalty per day for late submission. However, you will have 4 days throughout the whole semester to submit late without penalty. Note that the grace period is calculated by days instead of hours. If you submit the homework one minute after the deadline, one late day will be counted. Likewise, if you submit one minute after the deadline, the 10% penaly will be imposed if not using the grace period.\n","\n"]}]}